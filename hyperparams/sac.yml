# Tuned
MountainCarContinuous-v0:
  # env_wrapper: utils.wrappers.PlotActionWrapper
  normalize: True
  n_timesteps: !!float 50000
  policy: 'MlpPolicy'
  learning_rate: !!float 3e-4
  buffer_size: 50000
  batch_size: 256
  ent_coef: 'auto'
  gradient_steps: 128
  train_freq: 128
  learning_starts: 0
  use_sde: True
  policy_kwargs: "dict(log_std_init=-1, net_arch=[64, 64])"

Pendulum-v0:
  # env_wrapper:
  #   - utils.wrappers.DelayedRewardWrapper:
  #       delay: 5
  #   - utils.wrappers.HistoryWrapper:
  #       horizon: 5
  n_timesteps: 20000
  policy: 'MlpPolicy'
  learning_rate: !!float 1e-3
  use_sde: True
  n_episodes_rollout: 1
  gradient_steps: -1
  policy_kwargs: "dict(log_std_init=-2, net_arch=[64, 64])"

LunarLanderContinuous-v2:
  n_timesteps: !!float 5e5
  policy: 'MlpPolicy'
  batch_size: 256
  learning_starts: 1000

# Tuned
BipedalWalker-v2:
  n_timesteps: !!float 5e5
  policy: 'MlpPolicy'
  learning_rate: !!float 7.3e-4
  buffer_size: 300000
  batch_size: 256
  ent_coef: 'auto'
  gamma: 0.98
  tau: 0.02
  train_freq: 64
  gradient_steps: 64
  learning_starts: 10000
  use_sde: True
  policy_kwargs: "dict(log_std_init=-3.3, net_arch=[400, 300])"

# Almost tuned
BipedalWalkerHardcore-v2:
  n_timesteps: !!float 3e7
  policy: 'MlpPolicy'
  learning_rate: lin_7.3e-4
  buffer_size: 1000000
  batch_size: 256
  ent_coef: 'auto'
  gamma: 0.99
  tau: 0.01
  train_freq: 64
  gradient_steps: 64
  learning_starts: 10000
  use_sde: True
  policy_kwargs: "dict(log_std_init=-3.3, net_arch=[400, 300], use_expln=True)"

# Tuned
HalfCheetahBulletEnv-v0:
  # env_wrapper:
  #   - utils.wrappers.TimeFeatureWrapper
  #   - utils.wrappers.DelayedRewardWrapper:
  #       delay: 10
  #   - utils.wrappers.HistoryWrapper:
  #       horizon: 10
  env_wrapper: utils.wrappers.TimeFeatureWrapper
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  learning_rate: !!float 7.3e-4
  buffer_size: 300000
  batch_size: 256
  ent_coef: 'auto'
  gamma: 0.98
  tau: 0.02
  train_freq: 64
  gradient_steps: 64
  learning_starts: 10000
  use_sde: True
  policy_kwargs: "dict(log_std_init=-3.3, net_arch=[400, 300])"

# Tuned
AntBulletEnv-v0:
  env_wrapper: utils.wrappers.TimeFeatureWrapper
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  learning_rate: !!float 7.3e-4
  buffer_size: 300000
  batch_size: 256
  ent_coef: 'auto'
  gamma: 0.98
  tau: 0.02
  train_freq: 64
  gradient_steps: 64
  learning_starts: 10000
  use_sde: True
  policy_kwargs: "dict(log_std_init=-3.3, net_arch=[400, 300])"

HopperBulletEnv-v0:
  env_wrapper: utils.wrappers.TimeFeatureWrapper
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  learning_rate: lin_7.3e-4
  buffer_size: 300000
  batch_size: 256
  ent_coef: 'auto'
  gamma: 0.98
  tau: 0.02
  train_freq: 64
  gradient_steps: 64
  learning_starts: 10000
  use_sde: True
  policy_kwargs: "dict(log_std_init=-3.3, net_arch=[400, 300])"

Walker2DBulletEnv-v0:
  env_wrapper: utils.wrappers.TimeFeatureWrapper
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  learning_rate: lin_7.3e-4
  buffer_size: 300000
  batch_size: 256
  ent_coef: 'auto'
  gamma: 0.98
  tau: 0.02
  train_freq: 64
  gradient_steps: 64
  learning_starts: 10000
  use_sde: True
  policy_kwargs: "dict(log_std_init=-3.3, net_arch=[400, 300])"

# Walker2DBulletEnv-v0:
#   env_wrapper: utils.wrappers.TimeFeatureWrapper
#   n_timesteps: !!float 1e6
#   policy: 'MlpPolicy'
#   learning_rate: lin_7.3e-4
#   buffer_size: 200000
#   batch_size: 256
#   ent_coef: 'auto'
#   gamma: 0.95
#   tau: 0.02
#   train_freq: 16
#   gradient_steps: 16
#   learning_starts: 10000
#   use_sde: True
#   policy_kwargs: "dict(log_std_init=-2.37, net_arch=[400, 300])"

# Tuned
ReacherBulletEnv-v0:
  env_wrapper: utils.wrappers.TimeFeatureWrapper
  n_timesteps: !!float 3e5
  policy: 'MlpPolicy'
  learning_rate: !!float 7.3e-4
  buffer_size: 300000
  batch_size: 256
  ent_coef: 'auto'
  gamma: 0.98
  tau: 0.02
  train_freq: 64
  gradient_steps: 64
  learning_starts: 10000
  use_sde: True
  policy_kwargs: "dict(log_std_init=-3.3, net_arch=[400, 300])"

HumanoidBulletEnv-v0:
  normalize: "{'norm_obs': True, 'norm_reward': False}"
  n_timesteps: !!float 2e7
  policy: 'MlpPolicy'
  learning_rate: lin_3e-4
  buffer_size: 1000000
  batch_size: 64
  ent_coef: 'auto'
  train_freq: 1
  gradient_steps: 1
  learning_starts: 1000

# Tuned
InvertedDoublePendulumBulletEnv-v0:
  env_wrapper: utils.wrappers.TimeFeatureWrapper
  n_timesteps: !!float 5e5
  policy: 'MlpPolicy'
  learning_rate: !!float 7.3e-4
  buffer_size: 300000
  batch_size: 256
  ent_coef: 'auto'
  gamma: 0.98
  tau: 0.02
  train_freq: 64
  gradient_steps: 64
  learning_starts: 10000
  use_sde: True
  policy_kwargs: "dict(log_std_init=-3.3, net_arch=[400, 300])"

# Tuned
InvertedPendulumSwingupBulletEnv-v0:
  env_wrapper: utils.wrappers.TimeFeatureWrapper
  n_timesteps: !!float 3e5
  policy: 'MlpPolicy'
  learning_rate: !!float 7.3e-4
  buffer_size: 300000
  batch_size: 256
  ent_coef: 'auto'
  gamma: 0.98
  tau: 0.02
  train_freq: 64
  gradient_steps: 64
  learning_starts: 10000
  use_sde: True
  policy_kwargs: "dict(log_std_init=-3.3, net_arch=[400, 300])"

# To be tuned
MinitaurBulletEnv-v0:
  normalize: "{'norm_obs': True, 'norm_reward': False}"
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  learning_rate: lin_3e-4
  buffer_size: 1000000
  batch_size: 64
  ent_coef: 'auto'
  # ent_coef: 0.0003
  train_freq: 1
  gradient_steps: 1
  learning_starts: 1000

# To be tuned
MinitaurBulletDuckEnv-v0:
  # normalize: "{'norm_obs': True, 'norm_reward': False}"
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  learning_rate: lin_3e-4
  buffer_size: 1000000
  batch_size: 256
  ent_coef: 'auto'
  train_freq: 1
  gradient_steps: 1
  learning_starts: 1000
